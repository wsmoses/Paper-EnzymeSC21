\section{Introduction}
\label{sec:intro}

Automatic differentiation (AD) provides an accurate way of computing derivatives of mathematical functions that are implemented in computer programs. \emph{Gradients} (or \emph{adjoints}), a special case of derivatives for functions with one output and many inputs, have applications in optimization \cite{duchi2011adaptive}, uncertainty quantification \cite{ghanem2017handbook}, inverse design, stability analysis,%GAIL - nor refs?
and machine learning \cite{maclaurin2015gradient}. Reverse-mode AD  has been the tool of choice to compute these gradients for large applications with many input parameters.

% Automatic differentiation (AD) provides an accurate way of computing derivatives of multivariate vector function implementations $\bm{y} = f(\bm{x}),\, \REAL^n \mapsto \REAL^m$ with input $\bm{x}$ and output $\bm{y}$. With a variety of approaches for calculating these derivatives automatically, ranging from source transformation to operator overloading, this paper focuses on a compiler integrated solution through LLVM. There are two modes of differentiation. The tangent-mode generates the implementation of the Jacobian vector product $\dot{\bm{y}} = \nabla f(\bm{x}) \cdot \dot{\bm{x}}$, whereas reverse-mode AD computes the transposed Jacobian vector product $\bar{\bm{x}} = \bar{\bm{y}} \cdot \nabla f(\bm{x})^T$, with $\dot{}$ and $\bar{}$ denoting directional derivatives and adjoints, respectively.  Computing \emph{gradients} in reverse mode, a special case of derivatives with one output $m=1$ and many inputs $n$, is of particular usefulness for applications in optimization, uncertainty quantification, inverse design, stability analysis, and machine learning, as the resulting gradient implementation has a complexity of $\bigo{cost(f)}$, independent of $n$.

As the research community has been continuously pushing the boundary of the size of problems they want to solve, large-scale applications have had to leverage the latest in high-performance computing including distributed computation, parallelism, and accelerators. % Many  machine learning and scientific computing applications rely on kernels that are highly optimized for graphics processing units (GPUs). 
For many machine learning and scientific computing applications, this means relying on kernels that are highly optimized for graphics processing units (GPUs). 
% For many machine learning and scientific computing applications, this means optimizing your code for graphics processing units (GPUs).
%(for NVIDIA GPU's) or ROCm (for AMD GPU's)

\begin{figure}
    \centering
%\textbf{(a)}\\
\begin{minipage}[T]{0.95\linewidth}
\begin{code}
|\;|
void init(double* ar, int N, double val) {
  parallel_for(int i=0; i<N ; i++)
    // Concurrent reads of val
    ar[i] = val;
}

double gradient_init(double* ar, double* d_ar,
                     int N, double val) {
  double d_val = 0.0;
  parallel_for(int i=0; i<N ; i++)
    ar[i] = val;
  parallel_for(int i=0; i<N ; i++) {
    // Concurrent writes to d_val
    d_val += d_ar[i];                  |\Large\textcolor{red}{{\Lightning{}} race {{}\Lightning{}}}|
    d_ar[i] = 0.0;
  }
  return d_val;
}
|\;|
\end{code}
\end{minipage}
\vspace*{-3mm}
    \caption{
    A parallel initialize function (top) with a naive reverse mode AD gradient function (bottom) that does not take the parallelism into account.
    Consequently, the concurrent read of the variable \txtcode{val} causes a race in the reverse-mode gradient computation.
%    Without taking into consideration parallelism, a benign read race in the function \texttt{init} (a) leads to a write race in the reverse pass of the corresponding gradient computation \texttt{gradient\_init} (b).
}
    \label{fig:benign_read}
\end{figure}

While  considerable effort has  been expended to compute gradients of MPI and OpenMP programs (see Sec~\ref{sec:related} for related work), no AD tool has been presented to date that can compute gradients of CUDA or ROCm (AMD) kernels. The cause rests with both the GPU's parallelism and its complex performance characteristics. The biggest issue for both performance and correctness is due to the implied computational flow reversal of reverse-mode AD; every read becomes a write in the adjoint computation and vice versa. Consider the program \change{at the top of} Figure \ref{fig:benign_read}. It contains a simple parallel for loop that reads from the same variable \texttt{val} in all threads and sets each index of the output array \texttt{ar} to that value. Since all threads read the same value, there is a \textit{concurrent read access} on \texttt{val}, which does not impact the final result. Computing the gradient function of this program (i.e. the derivative of the input \texttt{val}), one must accumulate all of the partial derivatives of \texttt{val} generated by uses in the outputs. Such an action unfortunately leads to a \textit{write race} on the gradient \texttt{d\_val}, which may be updated by multiple threads at the same time. Special care must be taken to avoid undefined behavior and ensure the correctness of the gradient computation while preserving as much of its parallelism as possible.

GPUs often have relatively small amounts of memory per thread. Moreover, the memory of GPUs commonly has complex performance characteristics, with global memory being slow but large, shared memory being fast but small, 
and the use of certain types of memory preventing the simultaneous use of large thread counts. Potential remedies involve either sacrificing generality, by rewriting HPC applications in a differentiable domain-specific language (DSL), or resorting  to  approaches such as numerical differentiation. 

% require  , which is by nature more amendable to single instruction multiple data (SIMD) architectures like GPUs, because each component of the gradient can be independently computed. Despite this advantage, forward mode and vector forward mode both must evaluate the function a linear of a complexity of $\bigo{n\cdot cost(f)}$, where $n$ is the dimension of the gradient. Although reverse-mode automatic differentiation has a higher constant factor in its complexity of $\bigo{cost(f)}$, the gradient (derivative of all input parameters) amounts to a single function evaluation. This makes forward mode AD or finite differences intractable for programs with a large number of parameters such as neural networks, potential functions, and numerical simulations. 

To leverage the potential performance benefits of reverse-mode AD without sacrificing generality, one requires a tool that is  capable of both  handling the complex performance characteristics of GPU architectures and generating code that maintains the correctness of the gradient without sacrificing the inherent parallelism of the original program. 
Unlike many other tools, Enzyme\footnote{https://enzyme.mit.edu}~\cite{enzymeNeurips} performs AD alongside the traditional optimization pipeline by performing differentiation within the LLVM compiler~\cite{LLVM}. Thus,  we  can leverage existing code transformation infrastructure to build the requisite analyses and transformations for maintaining the correctness and performance of the corresponding gradient kernel. Furthermore, LLVM provides frontends for most commonly used languages including C/C++, Fortran, Julia, Rust, Swift, and TensorFlow and backends for different hardware architectures including CPU, NVIDIA GPUs \cite{rhodin2010ptx,holewinski2011ptx, gpucc}, and AMD GPUs, allowing us to build reverse-mode AD for multiple languages and architectures.

%\vfill\break
Overall, our paper makes the following contributions:
\begin{itemize}
    \item An algorithm for correctly generating gradients of GPU kernels and a corresponding proof sketch of correctness
    \item An extension to the Enzyme AD engine for LLVM that can generate gradients for GPU kernels written in either CUDA (NVIDIA) or ROCm (AMD)
    \item A collection of optimization passes for Enzyme/LLVM that allow generated GPU gradients to run efficiently on modern hardware
    \item A study demonstrating, for the first time, the feasibility of reverse-mode automatic differentiation of GPU kernels through the use of GPU and AD-specific optimizations (cach\-ing and re\-computation).
\end{itemize}

% Rewritten into the above
% While there are methods for obtaining Jacobians, Hessians and higher-order derivatives for arbitrarily-shaped functions, we focus in this work on gradient or adjoint computations which ask for the derivative of many inputs with respect a few outputs. This is an important special case, since many functions of practical interest have only one output  -- often a cost function, misfit, or similar -- and a large number of inputs such as design parameters, neural network weights, initial state variables, etc. As a result, gradient computations form the core of algorithms such as neural networks training, design optimization, and potential-based scientific simulation. The \emph{reverse mode} of AD is able to compute the derivative of all input gradient with a constant factor run time overhead. This is in contrast to simpler approaches like finite differences, also known as numerical differentiation, and forward-mode AD which require at least one perturbed function evaluation per input and thus have a run time overhead that scales linearly with the input size.

% Many applications for which gradients are required have been optimized for performance and use parallel hardware through MPI, OpenMP, CUDA, and ROCm (AMD). Consequently, it is desirable to have AD approaches that can handle parallel input programs. While there has been considerable effort in the past to compute gradients of MPI and OpenMP programs~\cite{5161165}, no AD tool has been presented to date that can compute gradients of CUDA or ROCm (AMD) kernels. Generally, gradient computation of parallel programs is known to be a hard problem [TODO cite], since reverse mode AD introduces additional data dependencies and potential race conditions. Efficient reverse mode AD of GPU kernels is even more challenging due to the severely constrained memory size and the need for non-trivial optimizations that do not easily carry over to the gradient computation.

% In this work, we present an extension to the freely available, open source Enzyme AD tool~\footnote{https://enzyme.mit.edu} that enables gradient computation of CUDA (NVIDIA) and ROCm (AMD) kernels through reverse mode AD. Enzyme is a plugin to the LLVM compiler that previously supported sequential programs written, for example, in C/C++, Julia, or Rust. Our extension combines existing LLVM optimizations with several new analyses and code transformations to efficiently compute gradients, which requires to resolve potential data race issues, differentiate parallel control (syncthreads), CUDA intrinsics (e.g. threadIdx.x), and handle shared memory. Our extension is now part of the latest Enzyme version and enabled by default.

% Eval description moved to results

% if we need space I'd be ok cutting the traditional systems paper layout of sections -wm

% The rest of the paper is structured as follows.
% We briefly summarize related work
% in Section~\ref{sec:related}.
% In Section~\ref{sec:method},
% we explain our overall methodology
% and its implementation.
% Section~\ref{sec:opt} describes the code analysis and optimization approaches that we use for better performance. We evaluate the performance of our approach in Section~\ref{sec:eval}
% and conclude with final remarks in Section~\ref{sec:conclusion}.

%\begin{itemize}
%    \item By extending Enzyme, to make it the first AD tool to reverse diff CUDA kernels
%        \item Race resolution in reverse
%        \item Sync <--
%        \item Misc CUDA [threadIdx, shared mem, etc]
%        \item Optimize to fit in GPU [mem] <--
%
%    \item Parallel Enzyme (GPU) is based on LLVM (C++ + Julia)
%
%    \item Gradients are useful in science and ML
%        \item Permeating new hybrid constructions in the sciences such as diff programming, neural PDEs etc.
%        
%    \item A few words about existing AD approaches
%        \item Fwd diff
%        \item DSL
%        \item set of known functions
%        
%    \item x Y speedups in our test cases, name test cases
%\end{itemize}
%
%Sentence fragments from the abstract:
%\begin{itemize}
%    \item We present an extension to Enzyme capable of modeling and differentiating parallel, and specifically GPU programs.
%    \item By working at the compiler level we are able to create a single infrastructure capable of differentiating multiple types of parallelism.
%    \item This involves preventing races in the reverse pass.
%    \item We further demonstrate the need to combine traditional optimizations with differentiation by both an ablation analysis of applying existing LLVM optimizations, as well as new AD-aware optimizations.
%    \item We present an extension to Enzyme capable of modeling and differentiating parallel, and specifically GPU programs.
%    \item In this paper, we present an extended version of Enzyme that is the first automatic differentiation tool that can generate gradients (reverse mode AD) of CUDA kernels.
%\end{itemize}
%
%Automatic differentiation (AD) is key to training neural networks, bayesian inference, and scientific computing. Applying these techniques requires rewriting code in a specific machine learning framework or manually providing derivatives. This talk presents Enzyme, a high performance automatic differentiation (AD) compiler plugin for the LLVM compiler capable of synthesizing gradients of programs expressed in the LLVM intermediate representation (IR). Enzyme differentiates programs in any language whose compiler targets LLVM including C/C++, Fortran, Julia, Rust, Swift, etc., thereby providing native AD capabilities in these languages with state of the art performance. Unlike traditional tools, Enzyme performs AD on optimized IR. On a machine-learning benchmark suite, AD on optimized IR achieves a geometric mean speedup of 4.5x over AD on IR before optimization. We discuss ongoing work into AD-specific optimization, performing parallel AD for CPU and GPU, and language integration.
%
%Enzyme enables differentiation of CPU programs without rewriting them in a DSL.
%Similarly, GPU programs cannot currently be differentiated without being rewritten in a differentiable language (e.g. PyTorch).
%Enzyme enables reverse-mode AD of general existing GPU programs by:
%Resolving potential data race issues
%Differentiating parallel control (syncthreads)
%Differentiating CUDA intrinsics (e.g. threadIdx.x /llvm.nvvm.read.ptx.sreg.tid.x)
%Handling shared memory
%
%Benign read race in forward pass => Write race in reverse pass (undefined behavior)
%
%
%%%% Neurips 
%%% Applying differentiable programming techniques and machine learning algorithms to foreign programs requires developers to either rewrite their code in a machine learning framework, or otherwise provide derivatives of the foreign code. This paper presents Enzyme, a high-performance automatic differentiation (AD) compiler plugin for the LLVM compiler framework capable of synthesizing gradients of statically analyzable programs expressed in the LLVM intermediate representation (IR). Enzyme synthesizes gradients for programs written in any language whose compiler targets LLVM IR including C, C++, Fortran, Julia, Rust, Swift, MLIR, etc., thereby providing native AD capabilities in these languages. Unlike traditional source-to-source and operator-overloading tools, Enzyme performs AD on optimized IR. On a machine-learning focused benchmark suite including Microsoft's ADBench, AD on optimized IR achieves a geometric mean speedup of 4.5x over AD on IR before optimization allowing Enzyme to achieve state-of-the-art performance. Packaging Enzyme for PyTorch and TensorFlow provides convenient access to gradients of foreign code with state-of-the art performance, enabling foreign code to be directly incorporated into existing machine learning workflows.
%
%
%%%
%%% New content above this line
%%%
%AD is important.
%AD for parallel and HPC applications is also important.
%Many tools try to do this, but Enzyme is very different.
%We show how great our tool is on the following test cases.
%Our paper is structured as follows.